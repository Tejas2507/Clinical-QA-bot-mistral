{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12420459,"sourceType":"datasetVersion","datasetId":7812389}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n    !pip install --no-deps unsloth\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T12:02:54.561465Z","iopub.execute_input":"2025-08-15T12:02:54.562147Z","iopub.status.idle":"2025-08-15T12:03:00.842836Z","shell.execute_reply.started":"2025-08-15T12:02:54.562104Z","shell.execute_reply":"2025-08-15T12:03:00.841948Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport re\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T12:20:48.521610Z","iopub.execute_input":"2025-08-15T12:20:48.522057Z","iopub.status.idle":"2025-08-15T12:20:48.525526Z","shell.execute_reply.started":"2025-08-15T12:20:48.522037Z","shell.execute_reply":"2025-08-15T12:20:48.524934Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch\nfrom unsloth import FastLanguageModel\n\n# Define the model details\nmax_seq_length = 4096\ndtype = None  # Autodetect\nload_in_4bit = True # Use 4-bit quantization for memory efficiency\n\n# Load the model with your LoRA adapters merged in\n# Unsloth's `from_pretrained` can directly load LoRA adapters from the Hub.\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"Tejas1615/medical-mstrl7b-lora_adapters_final_16b\", # üöÄ YOUR MODEL REPO ID\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\nprint(\"‚úÖ Successfully loaded your finetuned model from the Hugging Face Hub!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T12:03:04.175576Z","iopub.execute_input":"2025-08-15T12:03:04.176215Z","iopub.status.idle":"2025-08-15T12:04:10.688190Z","shell.execute_reply.started":"2025-08-15T12:03:04.176183Z","shell.execute_reply":"2025-08-15T12:04:10.687064Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-08-15 12:03:17.270949: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755259397.513202      60 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755259397.572223      60 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ü¶• Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.8.5: Fast Mistral patching. Transformers: 4.52.4.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ace79315355b4491bb859b90eabd4a23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/155 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5626de9e02348d5b76122a7a5459a2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b64f42d0e5e4840897a642600a05dc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"148f2231de6942a3902909af87625d91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5539b474c5ec461ba712a592d7fa78a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acdfe164bc1743468ab455295485a3af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/336M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11b8d057fc844cce83808c59a4a391bf"}},"metadata":{}},{"name":"stderr","text":"Unsloth 2025.8.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Successfully loaded your finetuned model from the Hugging Face Hub!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"prompt_template = \"\"\"<s>[INST] You are an expert medical assistant. Analyze the following multiple-choice question and provide the single best answer.\n\nQuestion: {question}\n\nOptions:\n{options}\n\nWhat is the correct answer? [/INST]\"\"\"\n\ndef format_inference_prompt(example):\n    \"\"\"\n    Formats a single example from your test JSONL files into the Mistral\n    instruction format.\n    \"\"\"\n    try:\n        question = example[\"question\"].strip()\n        # The test files have options as a dictionary, so we format them into a string\n        options_str = \"\\n\".join([f\"{key}. {value}\" for key, value in example[\"options\"].items()])\n        return prompt_template.format(question=question, options=options_str)\n    except Exception as e:\n        print(f\"Error formatting example: {e}\")\n        return None\n\ndef parse_model_output(generated_text):\n    \"\"\"\n    Extracts the single capital letter (A, B, C, etc.) from the model's\n    full text response.\n    \"\"\"\n    # The model often outputs \"A. The answer is...\"\n    # This regex finds the first capital letter followed by a period.\n    match = re.search(r\"^\\s*([A-Z])\\.\", generated_text.strip())\n    if match:\n        return match.group(1)\n    \n    # Fallback: if the regex fails, take the first character.\n    # This handles cases where the model just outputs \"A\" or \"A The answer...\"\n    if len(generated_text.strip()) > 0:\n        return generated_text.strip()[0]\n        \n    # If all else fails, return a default value (e.g., 'A')\n    return 'A'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T12:14:26.072085Z","iopub.execute_input":"2025-08-15T12:14:26.072431Z","iopub.status.idle":"2025-08-15T12:14:26.079432Z","shell.execute_reply.started":"2025-08-15T12:14:26.072405Z","shell.execute_reply":"2025-08-15T12:14:26.078688Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\n\n# List of your test files\ntest_files = [\n    '/kaggle/input/godel-data-2/test_easy.jsonl',\n    '/kaggle/input/godel-data-2/test_medium.jsonl',\n    '/kaggle/input/godel-data-2/test_hard.jsonl',\n]\n\nall_predictions = []\nmodel.eval() # Set the model to evaluation mode\n\n# Loop through each test file\nfor file_path in test_files:\n    print(f\"\\nüöÄ Processing file: {os.path.basename(file_path)}\")\n    \n    try:\n        with open(file_path, 'r') as f:\n            test_examples = [json.loads(line) for line in f]\n    except FileNotFoundError:\n        print(f\"‚ö†Ô∏è File not found: {file_path}. Skipping.\")\n        continue\n\n    # Generate answers for each example with a progress bar\n    for example in tqdm(test_examples, desc=f\"Inferencing {os.path.basename(file_path)}\"):\n        prompt = format_inference_prompt(example)\n        if not prompt:\n            continue\n\n        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n\n        with torch.no_grad():\n            # --- ACCURACY IMPROVEMENT ---\n            # We set do_sample=False to make the output deterministic and choose\n            # the most likely answer instead of a creative one.\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=50, # We only need the first few tokens for the answer\n                do_sample=False,   # This is the key for accuracy\n                pad_token_id=tokenizer.eos_token_id # Suppress warning\n            )\n        \n        # Decode the full generated text\n        full_response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n        \n        # Extract only the answer part after the final [/INST] tag\n        answer_part = full_response.split(\"[/INST]\")[-1].strip()\n        \n        # Parse the letter from the answer\n        predicted_letter = parse_model_output(answer_part)\n        \n        all_predictions.append({\n            \"id\": example[\"id\"],\n            \"answer\": predicted_letter\n        })\n\n# Create the submission DataFrame\nsubmission_df = pd.DataFrame(all_predictions)\n\n# Save to submission.csv\nsubmission_df.to_csv(\"submission.csv\", index=False)\n\nprint(\"\\n\\n\" + \"=\"*80)\nprint(\"‚úÖ Inference complete! `submission.csv` has been generated.\")\nprint(\"=\"*80)\nprint(\"\\nHere's a preview of your submission file:\\n\")\nprint(submission_df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T12:20:54.166707Z","iopub.execute_input":"2025-08-15T12:20:54.167042Z","iopub.status.idle":"2025-08-15T12:26:25.964040Z","shell.execute_reply.started":"2025-08-15T12:20:54.167020Z","shell.execute_reply":"2025-08-15T12:26:25.963244Z"}},"outputs":[{"name":"stdout","text":"\nüöÄ Processing file: test_easy.jsonl\n","output_type":"stream"},{"name":"stderr","text":"Inferencing test_easy.jsonl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:21<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüöÄ Processing file: test_medium.jsonl\n","output_type":"stream"},{"name":"stderr","text":"Inferencing test_medium.jsonl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:57<00:00,  1.17s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nüöÄ Processing file: test_hard.jsonl\n","output_type":"stream"},{"name":"stderr","text":"Inferencing test_hard.jsonl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:13<00:00,  1.34s/it]","output_type":"stream"},{"name":"stdout","text":"\n\n================================================================================\n‚úÖ Inference complete! `submission.csv` has been generated.\n================================================================================\n\nHere's a preview of your submission file:\n\n                                     id answer\n0  0396d20b-11aa-4159-8286-a451371a99f0      C\n1  6ff719a1-8a31-4f2a-b717-007443c1bd3f      A\n2  3ef59327-18e4-4dd8-b482-666afc9a6833      A\n3  d73b4d17-0170-4f1a-bafd-6f3a037e022b      A\n4  1a7805f1-c0f0-465a-b996-bb5047cb9501      A\n5  7d70e718-bef4-4e7a-8781-87fb2a5d52d6      A\n6  afea498d-f47c-4f18-b787-ed60e7dfbdc4      A\n7  feffb7ba-1e94-4736-8223-154335ab7581      C\n8  2c3b5990-1b2a-47e7-ac46-35e4dddff0e3      A\n9  ed0e3c88-e875-42a7-b9f9-4b75c492c6d5      A\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}