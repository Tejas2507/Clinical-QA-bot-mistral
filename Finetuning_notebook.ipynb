{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12420459,"sourceType":"datasetVersion","datasetId":7812389}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:21:53.550691Z","iopub.execute_input":"2025-08-15T07:21:53.551492Z","iopub.status.idle":"2025-08-15T07:21:53.849902Z","shell.execute_reply.started":"2025-08-15T07:21:53.551461Z","shell.execute_reply":"2025-08-15T07:21:53.849230Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/godel-data-2/test_medium.jsonl\n/kaggle/input/godel-data-2/train_seq2seq.jsonl\n/kaggle/input/godel-data-2/val_seq2seq.jsonl\n/kaggle/input/godel-data-2/test_easy.jsonl\n/kaggle/input/godel-data-2/test_hard.jsonl\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n    !pip install --no-deps unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:21:53.851181Z","iopub.execute_input":"2025-08-15T07:21:53.851559Z","iopub.status.idle":"2025-08-15T07:22:10.831759Z","shell.execute_reply.started":"2025-08-15T07:21:53.851535Z","shell.execute_reply":"2025-08-15T07:22:10.830910Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:22:10.832693Z","iopub.execute_input":"2025-08-15T07:22:10.832940Z","iopub.status.idle":"2025-08-15T07:22:10.986218Z","shell.execute_reply.started":"2025-08-15T07:22:10.832903Z","shell.execute_reply":"2025-08-15T07:22:10.985622Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:22:10.987571Z","iopub.execute_input":"2025-08-15T07:22:10.988439Z","iopub.status.idle":"2025-08-15T07:22:14.298815Z","shell.execute_reply.started":"2025-08-15T07:22:10.988420Z","shell.execute_reply":"2025-08-15T07:22:14.298121Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.4)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import unsloth\nimport torch\nimport json\nfrom datasets import Dataset, load_dataset\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\nfrom unsloth import FastLanguageModel\nfrom unsloth.chat_templates import get_chat_template\n\n# === 1. Load Model and Tokenizer ===\nmax_seq_length = 4096\ndtype = torch.float16\nload_in_4bit = True\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/mistral-7b-instruct-v0.2\", # Using the Unsloth optimized Mistral model\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\n# Apply Phi-3.5 native chat template\n# tokenizer = get_chat_template(\n#     tokenizer, \n#     chat_template = \"phi-3.5\"\n# )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:22:14.299825Z","iopub.execute_input":"2025-08-15T07:22:14.300096Z","iopub.status.idle":"2025-08-15T07:23:48.424320Z","shell.execute_reply.started":"2025-08-15T07:22:14.300052Z","shell.execute_reply":"2025-08-15T07:23:48.423749Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-08-15 07:22:37.387754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755242557.763591      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755242557.868637      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.8.5: Fast Mistral patching. Transformers: 4.52.4.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d550eada49d04c5591a7b9112586b7b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/155 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"529c9f70a5f04c11a69cadfec4b02f17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"621fd808bcdb4ab9b0f48ac2776cf235"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61503666e1bf44ef87c56b5077a6cf6b"}},"metadata":{}},{"name":"stdout","text":"{\"timestamp\":\"2025-08-15T07:23:42.954337Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 504. Retrying...\",\"request_id\":\"\"},\"filename\":\"/home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n{\"timestamp\":\"2025-08-15T07:23:42.954396Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #0. Sleeping 2.786074221s before the next attempt\"},\"filename\":\"/root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac4f8f900e534d9db587de82fa806658"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d431b493cb5471893d015701835bbd3"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 32,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 96,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:23:48.424989Z","iopub.execute_input":"2025-08-15T07:23:48.425215Z","iopub.status.idle":"2025-08-15T07:23:55.341561Z","shell.execute_reply.started":"2025-08-15T07:23:48.425197Z","shell.execute_reply":"2025-08-15T07:23:55.340950Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.8.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import json\n\n# *** PROMPT TEMPLATE CHANGED FOR MISTRAL ***\n# This new function parses your data and formats it with the correct [INST] tags.\ndef format_user_mcq_data_for_mistral(examples):\n    \"\"\"\n    Formats your specific medical MCQ dataset for the Mistral Instruct chat template.\n    \"\"\"\n    texts = []\n    # System prompt provides the initial instruction\n    system_prompt = \"You are a helpful medical assistant that answers multiple choice questions accurately. Provide the correct letter option followed by the full text of that option.\"\n    \n    batch_size = len(examples['input'])\n    for i in range(batch_size):\n        try:\n            full_input = examples['input'][i]\n            full_output = examples['output'][i]\n\n            input_parts = full_input.split('\\n')\n            question = input_parts[0].strip()\n            options_str = \"\\n\".join(input_parts[1:])\n            assistant_response = full_output.replace(\" is the correct answer.\", \"\").strip()\n\n            # Create the Mistral-formatted prompt string\n            # The <s> and </s> tokens are beginning/end of sequence tokens.\n            # [INST] and [/INST] are the instruction markers.\n            prompt = f\"<s>[INST] {system_prompt}\\n\\nQuestion: {question}\\n\\nOptions:\\n{options_str}\\n\\nWhat is the correct answer? [/INST] {assistant_response}</s>\"\n            texts.append(prompt)\n\n        except Exception as e:\n            print(f\"Error processing example {i}: {e}\")\n            texts.append(\"\") # Append empty string on error\n\n    return {\"text\": texts}\n\n# Load your dataset using the 'json' format for .jsonl files\ntrain_file_path = \"/kaggle/input/godel-data-2/train_seq2seq.jsonl\"\nval_file_path = \"/kaggle/input/godel-data-2/val_seq2seq.jsonl\"\n\nprint(f\"Loading training data from: {train_file_path}\")\nraw_train_dataset = load_dataset(\"json\", data_files=train_file_path, split=\"train\")\n\nprint(f\"Loading validation data from: {val_file_path}\")\nraw_val_dataset = load_dataset(\"json\", data_files=val_file_path, split=\"train\")\n\n# Apply the new formatting function\ntrain_dataset = raw_train_dataset.map(\n    format_user_mcq_data_for_mistral,\n    batched=True,\n    remove_columns=raw_train_dataset.column_names,\n    desc=\"Formatting training examples\"\n)\nval_dataset = raw_val_dataset.map(\n    format_user_mcq_data_for_mistral,\n    batched=True,\n    remove_columns=raw_val_dataset.column_names,\n    desc=\"Formatting validation examples\"\n)\n\n# Filter out any examples that failed to parse\ntrain_dataset = train_dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\nval_dataset = val_dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n\n# *** DATASET SIZE LIMITED TO 20,000 EXAMPLES ***\nif len(train_dataset) > 20000:\n    train_dataset = train_dataset.select(range(20000))\n\nprint(f\"\\nFinal training examples: {len(train_dataset)}\")\nprint(f\"Final validation examples: {len(val_dataset)}\")\n\nprint(\"\\n=== Sample Formatted Example ===\")\nif len(train_dataset) > 0:\n    print(train_dataset[0][\"text\"][:500] + \"...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:23:55.342354Z","iopub.execute_input":"2025-08-15T07:23:55.342623Z","iopub.status.idle":"2025-08-15T07:23:57.671342Z","shell.execute_reply.started":"2025-08-15T07:23:55.342599Z","shell.execute_reply":"2025-08-15T07:23:57.670748Z"}},"outputs":[{"name":"stdout","text":"Loading training data from: /kaggle/input/godel-data-2/train_seq2seq.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ddfa9b500244841ad8ad86f5478ef1b"}},"metadata":{}},{"name":"stdout","text":"Loading validation data from: /kaggle/input/godel-data-2/val_seq2seq.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4ad138bf5904b10b0761bd3e94f321e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Formatting training examples:   0%|          | 0/161505 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b15b481e32f24aff9e164bd47502988a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Formatting validation examples:   0%|          | 0/3297 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3668cdd515d14c8cb46463eda1277717"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/161505 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"024433ab172c4efbac950ca295c709c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/3297 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96e4dd416cd447f6b7cc06f6cdb2f350"}},"metadata":{}},{"name":"stdout","text":"\nFinal training examples: 20000\nFinal validation examples: 3297\n\n=== Sample Formatted Example ===\n<s>[INST] You are a helpful medical assistant that answers multiple choice questions accurately. Provide the correct letter option followed by the full text of that option.\n\nQuestion: Glycogenolysis is mediated by:\n\nOptions:\nA. Alpha 1 receptor\nB. Alpha l+beta2 receptor\nC. Beta 2 receptor\nD. Alpha 2 + beta 1 receptor\n\nWhat is the correct answer? [/INST] A. Alpha 1 receptor</s>...\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(\"\\n=== Sample Formatted Example ===\")\nif len(train_dataset) > 0:\n    print(train_dataset[0][\"text\"][:500] + \"...\")\n    print(f\"\\nEOS token: {repr(tokenizer.eos_token)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:23:57.672134Z","iopub.execute_input":"2025-08-15T07:23:57.672417Z","iopub.status.idle":"2025-08-15T07:23:57.677056Z","shell.execute_reply.started":"2025-08-15T07:23:57.672390Z","shell.execute_reply":"2025-08-15T07:23:57.676349Z"}},"outputs":[{"name":"stdout","text":"\n=== Sample Formatted Example ===\n<s>[INST] You are a helpful medical assistant that answers multiple choice questions accurately. Provide the correct letter option followed by the full text of that option.\n\nQuestion: Glycogenolysis is mediated by:\n\nOptions:\nA. Alpha 1 receptor\nB. Alpha l+beta2 receptor\nC. Beta 2 receptor\nD. Alpha 2 + beta 1 receptor\n\nWhat is the correct answer? [/INST] A. Alpha 1 receptor</s>...\n\nEOS token: '</s>'\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\n\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    eval_dataset = val_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False,\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        num_train_epochs = 1, # Set to 1 since we're using a subset\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 10, # Log loss every 10 steps\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\",\n        # *** EVALUATION STRATEGY UPDATED ***\n        eval_strategy = \"steps\", # Evaluate during training\n        eval_steps = 250,              # Evaluate every 250 steps\n        save_strategy = \"steps\",       # Save checkpoint at same interval\n        save_steps = 250,              # Save every 250 steps\n        save_total_limit = 3,          # Only keep the last 3 checkpoints\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:24:11.682897Z","iopub.execute_input":"2025-08-15T07:24:11.683206Z","iopub.status.idle":"2025-08-15T07:24:15.174032Z","shell.execute_reply.started":"2025-08-15T07:24:11.683183Z","shell.execute_reply":"2025-08-15T07:24:15.173495Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"]:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8d25f670761428988749da535f5230e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"]:   0%|          | 0/3297 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1e5a6d3cb234a48b464b9775e9bef78"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Start Training\nprint(\"Starting training...\")\ntrainer_stats = trainer.train()\nprint(\"Training completed!\")\n\n# Save Model\nmodel.save_pretrained(\"medical_mistral7b_lora\")\ntokenizer.save_pretrained(\"medical_mistral7b_lora\")\nprint(\"Model saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:34:50.594063Z","iopub.execute_input":"2025-08-15T07:34:50.594977Z","iopub.status.idle":"2025-08-15T11:51:58.150820Z","shell.execute_reply.started":"2025-08-15T07:34:50.594942Z","shell.execute_reply":"2025-08-15T11:51:58.149959Z"}},"outputs":[{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 20,000 | Num Epochs = 1 | Total steps = 1,250\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 83,886,080 of 7,325,618,176 (1.15% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1250/1250 4:16:39, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>250</td>\n      <td>0.839000</td>\n      <td>0.816462</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.754500</td>\n      <td>0.793674</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.727500</td>\n      <td>0.767383</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.754900</td>\n      <td>0.742772</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.743900</td>\n      <td>0.727826</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Unsloth: Not an error, but MistralForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"},{"name":"stdout","text":"Training completed!\nModel saved successfully!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\n\nmodel.push_to_hub(\"Tejas1615/medical-mstrl7b-lora_adapters_final_16b\", token=secret_value_0)\ntokenizer.push_to_hub(\"Tejas1615/medical-mstrl7b-lora_adapters_final_16b\", token=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T11:55:20.977223Z","iopub.execute_input":"2025-08-15T11:55:20.977935Z","iopub.status.idle":"2025-08-15T11:55:29.326516Z","shell.execute_reply.started":"2025-08-15T11:55:20.977909Z","shell.execute_reply":"2025-08-15T11:55:29.325808Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8282a58b95fd4f088d9ab01c1c4e317a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9789c23a44b44a9beaad240e14bd27e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/336M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbac0027731142af82568f4f71245608"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/Tejas1615/medical-mstrl7b-lora_adapters_final_16b\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"566f844702ea4d5ca727c219822d291a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15f31b4e3bd94799abe04f65b3c51198"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}